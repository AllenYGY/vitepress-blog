import{_ as a,c as t,a7 as i,o as r}from"./chunks/framework.Bfq_PhNx.js";const c=JSON.parse('{"title":"Parameter-Efficient Fine-Tuning","description":"","frontmatter":{"date":"2024-07-04T00:00:00.000Z","title":"Parameter-Efficient Fine-Tuning","status":"UNFINISHED","author":["AllenYGY"],"tags":["NOTE","DeepLearning","PEFT"],"publish":true},"headers":[],"relativePath":"posts/Deep Learning/Fine-Tuning.md","filePath":"posts/Deep Learning/Fine-Tuning.md","lastUpdated":null}'),n={name:"posts/Deep Learning/Fine-Tuning.md"};function o(l,e,s,d,p,m){return r(),t("div",null,[...e[0]||(e[0]=[i('<p>#DeepLearning #Fine-Tuning #PEFT</p><h1 id="parameter-efficient-fine-tuning" tabindex="-1">Parameter-Efficient Fine-Tuning <a class="header-anchor" href="#parameter-efficient-fine-tuning" aria-label="Permalink to &quot;Parameter-Efficient Fine-Tuning&quot;">​</a></h1><h2 id="method" tabindex="-1">Method <a class="header-anchor" href="#method" aria-label="Permalink to &quot;Method&quot;">​</a></h2><ul><li>Modified small part of parameters of pretrained models</li><li>Add extra parameters</li><li>Add increment of parameters of pretrained models <ul><li>LoRA, FACT</li></ul></li></ul><h2 id="modified-small-part-of-parameters-of-pretrained-models" tabindex="-1">Modified small part of parameters of pretrained models <a class="header-anchor" href="#modified-small-part-of-parameters-of-pretrained-models" aria-label="Permalink to &quot;Modified small part of parameters of pretrained models&quot;">​</a></h2><h3 id="bias-terms-fine-tuning-bitfit-微调偏置项" tabindex="-1">Bias-terms Fine-tuning BitFit 微调偏置项 <a class="header-anchor" href="#bias-terms-fine-tuning-bitfit-微调偏置项" aria-label="Permalink to &quot;Bias-terms Fine-tuning BitFit 微调偏置项&quot;">​</a></h3><p>Only modify (retraining) part of parameters of pretrained models</p><ul><li>Bias and the final linear layer</li></ul><blockquote><p>[!success]+ Pros</p><ol><li>Simple, but efficient, comparable to full model fine-tuning</li><li>Can learn downstream tasks sequentially, which helps to deploy efficiently</li><li>For each downstream task, only very small number of parameters should be stored</li></ol><ul><li>For BERT backbone, 0.1% parameters are modified (retrained)</li></ul></blockquote><h2 id="add-extra-parameters" tabindex="-1">Add extra parameters <a class="header-anchor" href="#add-extra-parameters" aria-label="Permalink to &quot;Add extra parameters&quot;">​</a></h2><h3 id="adapter-tuning" tabindex="-1">Adapter Tuning <a class="header-anchor" href="#adapter-tuning" aria-label="Permalink to &quot;Adapter Tuning&quot;">​</a></h3><div class="tip custom-block github-alert"><p class="custom-block-title">+ Idea</p><p>Add Adapter module to each layer of pretrain models</p></div><h3 id="prefix-tuning" tabindex="-1">Prefix Tuning <a class="header-anchor" href="#prefix-tuning" aria-label="Permalink to &quot;Prefix Tuning&quot;">​</a></h3><div class="tip custom-block github-alert"><p class="custom-block-title">+ Idea</p><p>Use prefixs, which can be learned and specific to given tasks, to mimic prompts</p></div><h3 id="prompt-tuning" tabindex="-1">Prompt Tuning <a class="header-anchor" href="#prompt-tuning" aria-label="Permalink to &quot;Prompt Tuning&quot;">​</a></h3><div class="tip custom-block github-alert"><p class="custom-block-title">+ Idea</p><p></p><ul><li>Construct a prompt for each task, concatenate the prompt with data to input to Large Model</li><li>Only add prompt (tokens) at input layer, no MLP A Simplified version of Prefix Tuning</li></ul></div><h2 id="add-increment-of-parameters-of-pretrained-models" tabindex="-1">Add increment of parameters of pretrained models <a class="header-anchor" href="#add-increment-of-parameters-of-pretrained-models" aria-label="Permalink to &quot;Add increment of parameters of pretrained models&quot;">​</a></h2><h3 id="lora" tabindex="-1">LoRA <a class="header-anchor" href="#lora" aria-label="Permalink to &quot;LoRA&quot;">​</a></h3><div class="info custom-block github-alert"><p class="custom-block-title">+ Cons of previous tuning methods</p><p></p><ul><li>Adapter Tuning adds Adapter layer in Transformer, which makes the model more deeper and increases the inference time</li><li>Prompt-based methods, such as Prefix Tuning and Prompt Tuning, are hard to train. In addition, the prompt token occupy the input space so as to decrease the number of available tokens</li></ul></div><h3 id="fact" tabindex="-1">FacT <a class="header-anchor" href="#fact" aria-label="Permalink to &quot;FacT&quot;">​</a></h3>',20)])])}const f=a(n,[["render",o]]);export{c as __pageData,f as default};
//# sourceMappingURL=posts_Deep Learning_Fine-Tuning.md.Bu5Raj9L.js.map

import{_ as a,c as i,j as t,a as n,o}from"./chunks/framework.Bfq_PhNx.js";const f=JSON.parse('{"title":"Attention","description":"","frontmatter":{"date":"2024-07-02T00:00:00.000Z","title":"Attention","status":"UNFINISHED","author":["AllenYGY"],"tags":["NOTE"],"publish":true},"headers":[],"relativePath":"posts/Deep Learning/NLP/Attention.md","filePath":"posts/Deep Learning/NLP/Attention.md","lastUpdated":null}'),l={name:"posts/Deep Learning/NLP/Attention.md"};function s(r,e,d,p,c,u){return o(),i("div",null,[...e[0]||(e[0]=[t("h1",{id:"attention",tabindex:"-1"},[n("Attention "),t("a",{class:"header-anchor",href:"#attention","aria-label":'Permalink to "Attention"'},"​")],-1),t("blockquote",null,[t("p",null,"[!question]+ What is attention? Attention Mechanism is an attempt to implement the same action of selectively concentrating on a few relevant things while ignoring others in deep neural networks")],-1),t("blockquote",null,[t("p",null,"[!question]+ Why is Attention needed in deep learning ?")],-1),t("h2",{id:"self-attention",tabindex:"-1"},[n("Self-Attention "),t("a",{class:"header-anchor",href:"#self-attention","aria-label":'Permalink to "Self-Attention"'},"​")],-1),t("ul",null,[t("li",null,"intra-attention")],-1)])])}const m=a(l,[["render",s]]);export{f as __pageData,m as default};
//# sourceMappingURL=posts_Deep Learning_NLP_Attention.md.CPaz4UZB.js.map

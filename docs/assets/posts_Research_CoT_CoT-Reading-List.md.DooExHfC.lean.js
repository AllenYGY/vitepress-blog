import{_ as e,c as r,a7 as t,o as i}from"./chunks/framework.Bfq_PhNx.js";const f=JSON.parse('{"title":"CoT Reading List","description":"","frontmatter":{"date":"2024-07-22T00:00:00.000Z","title":"CoT Reading List","status":"DONE","author":["AllenYGY"],"tags":["NOTE","DeepLearning","CoT"],"publish":true},"headers":[],"relativePath":"posts/Research/CoT/CoT-Reading-List.md","filePath":"posts/Research/CoT/CoT-Reading-List.md","lastUpdated":null}'),l={name:"posts/Research/CoT/CoT-Reading-List.md"};function o(n,a,s,g,h,d){return i(),r("div",null,[...a[0]||(a[0]=[t('<p>#ReadingList #CoT</p><h1 id="cot-reading-list" tabindex="-1">CoT-Reading-List <a class="header-anchor" href="#cot-reading-list" aria-label="Permalink to &quot;CoT-Reading-List&quot;">​</a></h1><h2 id="基础论文" tabindex="-1">基础论文 <a class="header-anchor" href="#基础论文" aria-label="Permalink to &quot;基础论文&quot;">​</a></h2><ul><li><a href="https://arxiv.org/abs/2201.11903" target="_blank" rel="noreferrer">Chain-of-Thought Prompting Elicits Reasoning in Large Language Models</a></li><li><a href="https://arxiv.org/abs/2205.11916" target="_blank" rel="noreferrer">Large Language Models are Zero-Shot Reasoners</a></li><li><a href="https://arxiv.org/abs/2210.03493" target="_blank" rel="noreferrer">Automatic Chain of Thought Prompting in Large Language Models</a></li></ul><h2 id="问题分解" tabindex="-1">问题分解 <a class="header-anchor" href="#问题分解" aria-label="Permalink to &quot;问题分解&quot;">​</a></h2><ul><li><a href="https://arxiv.org/abs/2205.10625" target="_blank" rel="noreferrer">Least-to-Most Prompting Enables Complex Reasoning in Large Language Models</a></li><li><a href="https://arxiv.org/abs/2210.03350" target="_blank" rel="noreferrer">Measuring and Narrowing the Compositionality Gap in Language Models</a></li></ul><h2 id="融合预测" tabindex="-1">融合预测 <a class="header-anchor" href="#融合预测" aria-label="Permalink to &quot;融合预测&quot;">​</a></h2><ul><li><a href="https://arxiv.org/abs/2203.11171" target="_blank" rel="noreferrer">Self-Consistency Improves Chain of Thought Reasoning in Language Models</a></li><li><a href="https://arxiv.org/abs/2302.12246" target="_blank" rel="noreferrer">Active Prompting with Chain-of-Thought for Large Language Models</a></li><li><a href="https://arxiv.org/abs/2207.00747" target="_blank" rel="noreferrer">Rationale-Augmented Ensembles in Language Models</a></li></ul><h2 id="生成-校验" tabindex="-1">生成-校验 <a class="header-anchor" href="#生成-校验" aria-label="Permalink to &quot;生成-校验&quot;">​</a></h2><ul><li><a href="https://arxiv.org/abs/2203.14465" target="_blank" rel="noreferrer">STaR: Self-Taught Reasoner Bootstrapping Reasoning With Reasoning</a></li><li><a href="https://arxiv.org/abs/2206.02336" target="_blank" rel="noreferrer">On the Advance of Making Language Models Better Reasoners</a></li></ul><h2 id="多语言" tabindex="-1">多语言 <a class="header-anchor" href="#多语言" aria-label="Permalink to &quot;多语言&quot;">​</a></h2><ul><li><a href="https://arxiv.org/abs/2210.03057" target="_blank" rel="noreferrer">Language Models are Multilingual Chain-of-Thought Reasoners</a></li></ul><h2 id="大模型背景" tabindex="-1">大模型背景 <a class="header-anchor" href="#大模型背景" aria-label="Permalink to &quot;大模型背景&quot;">​</a></h2><ul><li><a href="https://arxiv.org/abs/2204.02311" target="_blank" rel="noreferrer">PaLM: Scaling Language Modeling with Pathways</a></li><li><a href="https://arxiv.org/abs/2206.07682" target="_blank" rel="noreferrer">Emergent Abilities of Large Language Models</a></li><li><a href="https://arxiv.org/abs/2207.10342" target="_blank" rel="noreferrer">Language Model Cascades</a></li></ul><h2 id="points-to-know" tabindex="-1">Points to know <a class="header-anchor" href="#points-to-know" aria-label="Permalink to &quot;Points to know&quot;">​</a></h2><ul><li><a class="internal new" href="./Sampling Methods">Sampling Methods</a><ul><li>Top-k Sampling</li><li>Nucleus Sampling</li><li>Beam Search</li><li>Temperature Sampling</li></ul></li><li><a class="internal new" href="./Decoding Methods">Decoding Methods</a><ul><li>Greedy Decoding</li><li>Self-Consistency</li><li><a class="internal" href="/posts/Research/CoT/CoT-Decoding">CoT-Decoding</a></li></ul></li><li>Instruction-tuned</li><li>System I</li><li>System II</li></ul>',16)])])}const c=e(l,[["render",o]]);export{f as __pageData,c as default};
//# sourceMappingURL=posts_Research_CoT_CoT-Reading-List.md.DooExHfC.js.map

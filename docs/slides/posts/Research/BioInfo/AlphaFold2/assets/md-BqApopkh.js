import{o as s,c as o,k as u,e as l,aa as e,q as i,s as m,B as a}from"./modules/vue-DiJU6KAl.js";import{I as p}from"./slidev/default-WqMFEZt5.js";import{u as c,f as d}from"./slidev/context-DudrFUHc.js";import"./index-BiPVwedZ.js";import"./modules/shiki-DDe08fYK.js";const L={__name:"AlphaFold2.md__slidev_9",setup(f){const{$slidev:g,$nav:h,$clicksContext:n,$clicks:_,$page:v,$renderContext:A,$frontmatter:t}=c();return n.setup(),(x,r)=>(s(),o(p,i(m(a(d)(a(t),8))),{default:u(()=>[...r[0]||(r[0]=[l("h3",null,"Evoformer Block",-1),l("ul",null,[l("li",null,[e("与Transformer 不同 "),l("ul",null,[l("li",null,"Transformer 输入的是一段序列"),l("li",null,"Evoformer 在这里输入的是一个二维的矩阵")])])],-1),l("p",null,[l("img",{src:"https://cdn.jsdelivr.net/gh/ALLENYGY/ImageSpace@master/IMAGE/Research/AlphaFold/a.png",alt:"a"})],-1),l("p",null,[l("em",null,"MSA"),e("和"),l("em",null,"Pair"),e("都会进入多头自注意力模块")],-1),l("ul",null,[l("li",null,"具有残差连接和在元素作用上的MLP")],-1),l("p",null,[l("em",null,"Pair"),e("的部分还需要加入物理上三角不等式的处理")],-1),l("p",null,[l("strong",null,"与Transformer不同")],-1),l("ul",null,[l("li",null,[e("氨基酸对 "),l("em",null,"(Pair Representation)"),e(" 之间的信息会加入对序列的建模 "),l("ul",null,[l("li",null,"序列建模之后也会加入对氨基酸对的建模"),l("li",null,[e("自注意力机制做两次 "),l("ul",null,[l("li",null,[e("因为输入是一个二维的矩阵 "),l("img",{src:"https://cdn.jsdelivr.net/gh/ALLENYGY/ImageSpace@master/IMAGE/Research/AlphaFold/a.png",alt:"a"})])])])])])],-1)])]),_:1},16))}};export{L as default};
